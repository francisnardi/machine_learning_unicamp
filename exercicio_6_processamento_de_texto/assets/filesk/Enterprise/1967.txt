

my customers  lie lie afford lie (or little customer service they’ll need lie quickly pay us
they lie data have
at i thought just weird client told needed handle billion calls month massive data stre analysis comes huge price tag i clear truth came hoped ramp million calls day months reached optimistic goal they’d one hundredth data they’d originally claimed
it just client i’ve good rule thumb assume company one thousandth data say do
companies brag size datasets way fishermen brag size fish claim access endless terabytes information advantages obvious know better
based marketing materials data makes companies clairvoyant claim deep insights performance employees preferences customer base data means understanding people make decisions people buy motivates  right
 marketing materials like fishermen exaggerate companies fraction data claim typically small fraction fraction useful generating non trivial insight
why companies lie size data want feel like big dogs they’ve heard enormous reserves data collected likes amazon facebook google reach collect data  money buy  want feel (and outsiders think trend data analyst cathy o’neil noted recent blog post believe normal tech company sprinkle data google
 big companies use tiny fraction data collect
twitter processes  terabytes data day sounds intimidating small company trying extract consumer insights tweets  data actual content tweets twitter users create  million tweets day average tweet  characters simple math just  gigabytes actual text content day  half percent  terabytes
the pattern continues wikipedia largest repositories text internet  text data fit single usb music world fit  disk drive i  point big data big  good data smaller
if large datasets useless talk useless deep learning models separate signal noise finding patterns typically experts months codify  typical deep learning models work massive amounts labeled data labelling large dataset takes hundreds thousands dollars months time job corporate behemoth like facebook google smaller companies realize acquire massive data stores ca afford use
these companies better option value data have
true deep learning algorithms need large datasets  design make inferences small data just like humans using transfer learning train algorithm large dataset sending work small makes learning process   times effective
here just examples startups transfer learning business use
you  programmer advantage services blockspring lets users mash apis excel spreadsheets writing line code
with options available makes sense purchase big data terabyte  brag it
it clear future data big small
